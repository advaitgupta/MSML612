# Distilled student Transformer configuration (debug/fast version)
env_id: FlyCraft-v0
run_name: student_debug
output_dir: runs
seed: 7
n_envs: 8

policy: transformer
policy_kwargs:
  transformer_kwargs:
    embed_dim: 256
    depth: 4
    num_heads: 8
    dropout: 0.1
    memory_size: 64

ppo_kwargs:
  learning_rate: 1.0e-4
  n_steps: 256  # reduced for faster updates
  batch_size: 256  # reduced for faster updates
  n_epochs: 2  # fewer epochs
  gamma: 0.995
  gae_lambda: 0.95
  clip_range: 0.2

timesteps: 5000  # drastically reduced for quick runs
save_freq: 1000  # save more frequently
eval_freq: 500   # evaluate more frequently

# Knowledge-distillation settings
distillation_temperature: 2.0
distillation_alpha: 0.5
distillation_weight: 0.7
teacher_model: runs/teacher_large_v5/best_model.zip
